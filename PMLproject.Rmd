---
title: "Practical Machine Learning Project"
author: "Benjamin Kariuki"
date: "February 9, 2018"
output: 
  html_document: 
    keep_md: yes
---

We load the data and the R packages we will use in this project.
```{r}
library(tree)
library(randomForest)
har.train <- read.csv("~/pml-training.csv")
nas <- character()
```

The data consists of 19,622 observations of 160 variables. We first look for variables with missing values and the number of missing values per variable.

```{r}
for(j in seq_along(names(har.train))){
  if(sum(is.na(har.train[j]))>0){
    print(paste(names(har.train[j]), sum(is.na(har.train[j]))))
    nas[j] <- names(har.train[j])
  } 
}
```

Each of the variables with missing values has 19,216 missing values. That is, for each of these 67 variables only about 2% of the total number of observations have any values recorded. It is fairly reasonable, therefore, to omit these 67 variables in the model we will use for prediction.

```{r}

nas1 <- unique((na.omit(nas)))
length(nas1)
```

A close look at the remaining 93 variables reveals that there are quite a number of variables with invalid values. The #DIV/0! error is generated by MS Excel when a formula that generates a column involves division of a number by 0. 
```{r}
pml.train <- har.train
pml.train <- har.train[, -which(names(har.train)%in%nas1)]
str(pml.train)
```

It is best to eliminate variables with such invalid values as #DIV/0! and empty character values "".

```{r}
div <- character()

for(j in seq_along(names(pml.train))){
  if(sum(pml.train[j]=="#DIV/0!")){
    print(paste(names(pml.train[j]), sum(pml.train[j]=="#DIV/0!")))
    div[j] <- names(pml.train[j])
  } 
}

div1 <- unique((na.omit(div)))
pmlFinal.train <- pml.train[, -which(names(pml.train)%in%div1)]
```

We split the data into a training set and a validation set.

```{r}
set.seed(3843)
inBuild <- sample(1:nrow(pmlFinal.train), .7 * nrow(pmlFinal.train))
buildData <- pmlFinal.train[inBuild, ]
validation <- pmlFinal.train[-inBuild, ]
```

# Model Building

Before we build the predictive model we note that the predictor variable is "classe" and that the variable "X" is simply the id of the observation and as such should not be included in the model.

We first fit a classification tree to the data.
```{r}
tree.pml <- tree(classe~.-X, buildData)
summary(tree.pml)
```

If we use this classification tree to predict the values of "classe" in the validation set, we obtain a misclassification error rate of about 26%.

```{r}
tree.pred <- predict(tree.pml, newdata = validation, type = "class")
table(tree.pred, validation$classe)
mean(tree.pred!=validation$classe)
```

We now consider whether pruning the tree might lead to improved results. We use cross-validation to determine the optimal level of tree complexity.
```{r}
set.seed(3843)
cv.pml <- cv.tree(tree.pml, FUN=prune.misclass)
cbind(cv.pml$size, cv.pml$dev)
plot(cv.pml$size, cv.pml$dev, type = 'b', xlab = "Terminal Nodes", ylab = "Cross-validation Error")
```

The tree with 17 terminal nodes results in the lowest cross-validation error rate, with 4,252 cross-validation errors.

We prune the original tree to obtain the 17-node tree

```{r}
prune.pml <- prune.misclass(tree.pml, best = 17)
summary(prune.pml)
```

The pruned to tree leads to a slight reduction in the misclassification error rate in the validation set down to about 24%.

```{r}
tree.pred <- predict(prune.pml, newdata = validation, type = "class")
table(tree.pred, validation$classe)
mean(tree.pred!=validation$classe)
```

We try more complex model building methods in an attempt to improve prediction accuracy.

Applying bagging to our data, we provide the argument mtry = 58 to indicate that all 58 predictor variables should be considered for each split of the tree.

```{r}
set.seed(3843)
bag.pml <- randomForest(classe~.-X, data = buildData, mtry = 58, importance = TRUE)
bag.pml
```

The bagged model leads to a significant reduction in the misclassification error rate in the validation set as compared to classification trees. It comes down to about 0.1%

```{r}
yhat.bag <- predict(bag.pml, newdata = validation)
table(yhat.bag, validation$classe)
mean(yhat.bag!=validation$classe)
```

The random forests model leads to an even better performance in the validation set with a misclassification error rate of about 0.08%.

```{r}
set.seed(3843)
rf.pml <- randomForest(classe~.-X, data = buildData, importance = TRUE)
rf.pml
yhat.rf <- predict(rf.pml, newdata = validation)
table(yhat.rf, validation$classe)
mean(yhat.rf!=validation$classe)
```

A variable importance plot indicates that across all of the trees considered in the random forest, besides the variables representing time, the acceleromators on the belt at Euler angles roll and yaw were most important in correctly determining whether the  dumbbell exercises were performed exactly according to the specification (Class A) or 4 other common mistakes were made (Classes B to E) while performing the exercises.

```{r}
varImpPlot(rf.pml)
```